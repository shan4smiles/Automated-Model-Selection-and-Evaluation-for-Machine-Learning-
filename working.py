# -*- coding: utf-8 -*-
"""working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cEzavIAACAQVCxXJ8fauGtGvY7dHlGB1
"""

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
from bayes_opt import BayesianOptimization
from sklearn.preprocessing import StandardScaler

!pip install bayesian_optimization

# Set a seed for reproducibility
seed_value = 42
random.seed(seed_value)
np.random.seed(seed_value)

# Function to load dataset
def load_data(file):
    if file is not None:
        ext = file.name.split('.')[-1]
        if ext == 'csv':
            data = pd.read_csv(file)
        elif ext in ['xls', 'xlsx']:
            data = pd.read_excel(file)
        else:
            st.error("Unsupported file format")
            return None
        return data
    return None

# Function to identify if the task is classification or regression
def is_classification(y):
    return y.dtype == 'object' or len(np.unique(y)) < 20

# Function to optimize and evaluate classification models
def optimize_classification_models(X, y):
    # Splitting the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature Scaling
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Bayesian Optimization for SVM
    def svm_cv(C, gamma):
        model = SVC(C=C, gamma=gamma)
        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=10).mean()

    # Bayesian Optimization for Decision Tree
    def dt_cv(max_depth, min_samples_split, min_samples_leaf):
        model = DecisionTreeClassifier(max_depth=int(max_depth),
                                       min_samples_split=int(min_samples_split),
                                       min_samples_leaf=int(min_samples_leaf))
        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=10).mean()

    # Bayesian Optimization for Random Forest
    def rf_cv(max_depth, n_estimators, min_samples_split, min_samples_leaf):
        model = RandomForestClassifier(max_depth=int(max_depth),
                                       n_estimators=int(n_estimators),
                                       min_samples_split=int(min_samples_split),
                                       min_samples_leaf=int(min_samples_leaf))
        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=10).mean()

    # Bayesian Optimization for KNN
    def knn_cv(n_neighbors, leaf_size):
        model = KNeighborsClassifier(n_neighbors=int(n_neighbors),
                                     leaf_size=int(leaf_size))
        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=10).mean()

    # Bayesian Optimization for Logistic Regression
    def lr_cv(C):
        model = LogisticRegression(C=C, max_iter=1000)
        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=10).mean()

    # Bayesian Optimization for Gradient Boosting Classifier
    def gbc_cv(n_estimators, learning_rate, max_depth):
        model = GradientBoostingClassifier(n_estimators=int(n_estimators),
                                           learning_rate=learning_rate,
                                           max_depth=int(max_depth))
        return cross_val_score(model, X_train, y_train, scoring='accuracy', cv=10).mean()

    # Define the parameter bounds
    svm_bounds = {'C': (0.1, 10), 'gamma': (0.01, 1)}
    dt_bounds = {'max_depth': (1, 50), 'min_samples_split': (2, 20), 'min_samples_leaf': (1, 20)}
    rf_bounds = {'max_depth': (1, 50), 'n_estimators': (10, 300), 'min_samples_split': (2, 20), 'min_samples_leaf': (1, 20)}
    knn_bounds = {'n_neighbors': (1, 30), 'leaf_size': (10, 50)}
    lr_bounds = {'C': (0.01, 10)}
    gbc_bounds = {'n_estimators': (50, 500), 'learning_rate': (0.01, 1), 'max_depth': (1, 10)}

    # Optimize hyperparameters
    svm_opt = BayesianOptimization(f=svm_cv, pbounds=svm_bounds, random_state=1)
    dt_opt = BayesianOptimization(f=dt_cv, pbounds=dt_bounds, random_state=1)
    rf_opt = BayesianOptimization(f=rf_cv, pbounds=rf_bounds, random_state=1)
    knn_opt = BayesianOptimization(f=knn_cv, pbounds=knn_bounds, random_state=1)
    lr_opt = BayesianOptimization(f=lr_cv, pbounds=lr_bounds, random_state=1)
    gbc_opt = BayesianOptimization(f=gbc_cv, pbounds=gbc_bounds, random_state=1)

    svm_opt.maximize(init_points=2, n_iter=10)
    dt_opt.maximize(init_points=2, n_iter=10)
    rf_opt.maximize(init_points=2, n_iter=10)
    knn_opt.maximize(init_points=2, n_iter=10)
    lr_opt.maximize(init_points=2, n_iter=10)
    gbc_opt.maximize(init_points=2, n_iter=10)

    # Retrieve the best parameters
    best_svm_params = svm_opt.max['params']
    best_dt_params = dt_opt.max['params']
    best_rf_params = rf_opt.max['params']
    best_knn_params = knn_opt.max['params']
    best_lr_params = lr_opt.max['params']
    best_gbc_params = gbc_opt.max['params']

    # Train the best models
    best_svm = SVC(C=best_svm_params['C'], gamma=best_svm_params['gamma']).fit(X_train, y_train)
    best_dt = DecisionTreeClassifier(max_depth=int(best_dt_params['max_depth']),
                                     min_samples_split=int(best_dt_params['min_samples_split']),
                                     min_samples_leaf=int(best_dt_params['min_samples_leaf'])).fit(X_train, y_train)
    best_rf = RandomForestClassifier(max_depth=int(best_rf_params['max_depth']),
                                     n_estimators=int(best_rf_params['n_estimators']),
                                     min_samples_split=int(best_rf_params['min_samples_split']),
                                     min_samples_leaf=int(best_rf_params['min_samples_leaf'])).fit(X_train, y_train)
    best_knn = KNeighborsClassifier(n_neighbors=int(best_knn_params['n_neighbors']),
                                    leaf_size=int(best_knn_params['leaf_size'])).fit(X_train, y_train)
    best_lr = LogisticRegression(C=best_lr_params['C'], max_iter=1000).fit(X_train, y_train)
    best_gbc = GradientBoostingClassifier(n_estimators=int(best_gbc_params['n_estimators']),
                                          learning_rate=best_gbc_params['learning_rate'],
                                          max_depth=int(best_gbc_params['max_depth'])).fit(X_train, y_train)

    # Compare models based on accuracy
    models = {
        'SVM': best_svm,
        'Decision Tree': best_dt,
        'Random Forest': best_rf,
        'KNN': best_knn,
        'Logistic Regression': best_lr,
        'Gradient Boosting': best_gbc
    }
    accuracies = {name: cross_val_score(model, X_test, y_test, scoring='accuracy', cv=10).mean() for name, model in models.items()}

    # Select the best model
    best_model_name = max(accuracies, key=accuracies.get)
    best_model = models[best_model_name]

    test_accuracy = best_model.score(X_test, y_test)

    return best_model, best_model_name, accuracies[best_model_name], test_accuracy

def optimize_regression_models(X, y):
    # Splitting the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature Scaling
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train the Linear Regression model
    lr_model = LinearRegression().fit(X_train, y_train)

    # Evaluate the model based on mean squared error using cross-validation
    cv_error = -cross_val_score(lr_model, X_train, y_train, scoring='neg_mean_squared_error', cv=10).mean()

    # Compute the mean squared error on the test set
    test_error = mean_squared_error(y_test, lr_model.predict(X_test))

    return lr_model, cv_error, test_error

data = pd.read_csv('phish.csv')

if data is not None:
    print("Dataset loaded successfully!")
    print(data.head())

    # Assuming the last column is the target
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]

    if is_classification(y):
        print("Classification type target identified.")
        best_model, best_model_name, cv_accuracy, test_accuracy = optimize_classification_models(X, y)
        print(f"Best Model: {best_model_name} with a cross-validated accuracy of {cv_accuracy}")
        print(f"Test set accuracy of the best model ({best_model_name}): {test_accuracy}")
    else:
        print("Regression type target identified.")
        best_model, cv_accuracy, test_error = optimize_regression_models(X, y)
        print(f"Best Model: Linear Regression with a cross-validated accuracy of {cv_accuracy}")
        print(f"Test set mean squared error of the best model Linear Regression: {test_error}")

features = [-1,0,-1,0,-1,0,-11]

best_model.predict([features])